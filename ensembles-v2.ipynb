{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"First, let's create a synthetic dataset for classification:","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nfrom sklearn.datasets import make_classification\n# create dataframe\nX, y = make_classification(n_samples=1000,\n                           n_features=20,\n                           n_informative=15,\n                           n_redundant=5,\n                           random_state=1)\n# print the data classes info\nprint(f'''Main dataframe:\nNumber of samples: {X.shape[0]}\nNumber of features: {X.shape[1]}\nSamples by class:''')\ncounter = Counter(y)\nfor k, v in counter.items():\n    per = v / len(y) * 100\n    print('Class=%d, Count=%d, Percentage=%.1f%%' % (k, v, per))\n\n# Main dataframe:\n# Number of samples: 1000\n# Number of features: 20\n# Samples by class:\n# Class=0, Count=501, Percentage=50.1%\n# Class=1, Count=499, Percentage=49.9%","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-27T14:15:19.978751Z","iopub.execute_input":"2022-05-27T14:15:19.979396Z","iopub.status.idle":"2022-05-27T14:15:21.123254Z","shell.execute_reply.started":"2022-05-27T14:15:19.979293Z","shell.execute_reply":"2022-05-27T14:15:21.122087Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Then we split the data into training and test parts:","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# split data to train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, stratify=y, random_state=1)\n# print the data classes info\nprint(f'''\\nTrain dataframe:\nNumber of samples: {X_train.shape[0]}\nNumber of features: {X_train.shape[1]}\nSamples by class:''')\ncounter = Counter(y_train)\nfor k, v in counter.items():\n    per = v / len(y_train) * 100\n    print('Class=%d, Count=%d, Percentage=%.1f%%' % (k, v, per))\n\n# Train dataframe:\n# Number of samples: 500\n# Number of features: 20\n# Samples by class:\n# Class=0, Count=251, Percentage=50.2%\n# Class=1, Count=249, Percentage=49.8%","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:15:21.126328Z","iopub.execute_input":"2022-05-27T14:15:21.127111Z","iopub.status.idle":"2022-05-27T14:15:21.199350Z","shell.execute_reply.started":"2022-05-27T14:15:21.127049Z","shell.execute_reply":"2022-05-27T14:15:21.198158Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"And now let's write a function for training and evaluating classifiers:","metadata":{}},{"cell_type":"code","source":"import time\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n# define predictor function\ndef predictor(model):\n    tic = time.perf_counter()\n    # fit classifier\n    model.fit(X_train, y_train)\n    # get prediction\n    y_pred = model.predict(X_test)\n    # check results\n    accuracy = accuracy_score(y_test, y_pred)\n    matrix = confusion_matrix(y_test, y_pred)\n    # show results\n    print(f'''\\n{type(model).__name__} results:\nAccuracy:{accuracy * 100: 0.1f}%\n{matrix}''')\n    toc = time.perf_counter()\n    print(f\"Processed in {toc - tic: 0.4f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:15:21.201227Z","iopub.execute_input":"2022-05-27T14:15:21.201840Z","iopub.status.idle":"2022-05-27T14:15:21.210933Z","shell.execute_reply.started":"2022-05-27T14:15:21.201798Z","shell.execute_reply":"2022-05-27T14:15:21.210150Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"It's time to write some code that demonstrates how well Decision Trees perform with classification:","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n# call Decision Tree\npredictor(DecisionTreeClassifier(random_state=1))\n\n# DecisionTreeClassifier results:\n# Accuracy: 78.0%\n# [[200  50]\n#  [ 60 190]]\n# Processed in  0.0157 seconds","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:15:21.214398Z","iopub.execute_input":"2022-05-27T14:15:21.214948Z","iopub.status.idle":"2022-05-27T14:15:21.411565Z","shell.execute_reply.started":"2022-05-27T14:15:21.214784Z","shell.execute_reply":"2022-05-27T14:15:21.410633Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Bagging Ensemble.\nNow let's see how much this approach is better than a single tree:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\n# call Bagging Trees\npredictor(BaggingClassifier(random_state=1))\n\n# BaggingClassifier results:\n# Accuracy: 87.8%\n# [[225  25]\n#  [ 36 214]]\n# Processed in  0.1033 seconds","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:15:21.412778Z","iopub.execute_input":"2022-05-27T14:15:21.413332Z","iopub.status.idle":"2022-05-27T14:15:21.564809Z","shell.execute_reply.started":"2022-05-27T14:15:21.413291Z","shell.execute_reply":"2022-05-27T14:15:21.563823Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Can Random Forest give even better results?","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n# call Random Forest\npredictor(RandomForestClassifier(random_state=1))\n\n# RandomForestClassifier results:\n# Accuracy: 91.2%\n# [[234  16]\n#  [ 28 222]]\n# Processed in  0.2108 seconds","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:15:21.566623Z","iopub.execute_input":"2022-05-27T14:15:21.568423Z","iopub.status.idle":"2022-05-27T14:15:21.867003Z","shell.execute_reply.started":"2022-05-27T14:15:21.568373Z","shell.execute_reply":"2022-05-27T14:15:21.866020Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Extremely Randomized Trees.\nEven better?","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import ExtraTreesClassifier\n# call Extra Trees\npredictor(ExtraTreesClassifier(random_state=1))\n\n# ExtraTreesClassifier results:\n# Accuracy: 93.0%\n# [[238  12]\n#  [ 23 227]]\n# Processed in  0.1247 seconds","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:15:21.871324Z","iopub.execute_input":"2022-05-27T14:15:21.871673Z","iopub.status.idle":"2022-05-27T14:15:22.061544Z","shell.execute_reply.started":"2022-05-27T14:15:21.871642Z","shell.execute_reply":"2022-05-27T14:15:22.060364Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Adaptive Boosting (AdaBoost).\nSounds cool, but does it really work?","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\n# call Adaptive Boosting\npredictor(AdaBoostClassifier(random_state=1))\n\n# AdaBoostClassifier results:\n# Accuracy: 85.2%\n# [[220  30]\n#  [ 44 206]]\n# Processed in  0.1048 seconds","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:15:22.063072Z","iopub.execute_input":"2022-05-27T14:15:22.063452Z","iopub.status.idle":"2022-05-27T14:15:22.225430Z","shell.execute_reply.started":"2022-05-27T14:15:22.063420Z","shell.execute_reply":"2022-05-27T14:15:22.224321Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"Gradient Boosting.\nLet's see how much better this approach is:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\n# call Gradient Boosting\npredictor(GradientBoostingClassifier(random_state=1))\n\n# GradientBoostingClassifier results:\n# Accuracy: 91.4%\n# [[230  20]\n#  [ 23 227]]\n# Processed in  0.2346 seconds","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:15:22.226879Z","iopub.execute_input":"2022-05-27T14:15:22.227309Z","iopub.status.idle":"2022-05-27T14:15:22.611302Z","shell.execute_reply.started":"2022-05-27T14:15:22.227268Z","shell.execute_reply":"2022-05-27T14:15:22.610172Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Extreme Gradient Boosting (XGBoost).\nLet's check:","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier\n# call Extreme Gradient Boosting\npredictor(XGBClassifier(random_state=1))\n\n# XGBClassifier results:\n# Accuracy: 92.2%\n# [[235  15]\n#  [ 24 226]]\n# Processed in  1.4683 seconds","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:15:22.614419Z","iopub.execute_input":"2022-05-27T14:15:22.614858Z","iopub.status.idle":"2022-05-27T14:15:23.898309Z","shell.execute_reply.started":"2022-05-27T14:15:22.614815Z","shell.execute_reply":"2022-05-27T14:15:23.897311Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Histogram-Based GradientÂ Boosting.\nLet's see how it copes with our small dataset:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import HistGradientBoostingClassifier\n# call Histogram-Based Gradient Boosting\npredictor(HistGradientBoostingClassifier(random_state=1))\n\n# HistGradientBoostingClassifier results:\n# Accuracy: 92.4%\n# [[235  15]\n#  [ 23 227]]\n# Processed in  0.3195 seconds","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:15:23.899693Z","iopub.execute_input":"2022-05-27T14:15:23.900038Z","iopub.status.idle":"2022-05-27T14:15:24.996432Z","shell.execute_reply.started":"2022-05-27T14:15:23.899989Z","shell.execute_reply":"2022-05-27T14:15:24.995374Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"We will select all models that have shown more than 90% accuracy. There are 5 of them:","metadata":{}},{"cell_type":"code","source":"estimators = [('rf', RandomForestClassifier(random_state=1)),\n              ('et', ExtraTreesClassifier(random_state=1)),\n              ('gb', GradientBoostingClassifier(random_state=1)),\n              ('xgb', XGBClassifier(random_state=1)),\n              ('hgb', HistGradientBoostingClassifier(learning_rate=1))]","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:15:24.997615Z","iopub.execute_input":"2022-05-27T14:15:24.998019Z","iopub.status.idle":"2022-05-27T14:15:25.003290Z","shell.execute_reply.started":"2022-05-27T14:15:24.997987Z","shell.execute_reply":"2022-05-27T14:15:25.002448Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Stack of estimators with a final classifier (Stacking).\nAnd now let's find out if the ensemble of ensembles can give the result above:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\n# call Stacked generalization\npredictor(StackingClassifier(estimators=estimators, final_estimator=LogisticRegression()))\n\n# StackingClassifier results:\n# Accuracy: 92.6%\n# [[239  11]\n#  [ 26 224]]\n# Processed in  2.8881 seconds","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:15:25.004653Z","iopub.execute_input":"2022-05-27T14:15:25.005623Z","iopub.status.idle":"2022-05-27T14:15:32.179438Z","shell.execute_reply.started":"2022-05-27T14:15:25.005583Z","shell.execute_reply":"2022-05-27T14:15:32.178613Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Hard Voting Classifier.\nCan this approach be better than Stacking? Let's find out!","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n# call Soft Majority Rule classifier\npredictor(VotingClassifier(estimators=estimators, voting='hard'))\n\n# VotingClassifier results:\n# Accuracy: 92.6%\n# [[238  12]\n#  [ 25 225]]\n# Processed in  0.5794 seconds","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:15:32.180653Z","iopub.execute_input":"2022-05-27T14:15:32.181128Z","iopub.status.idle":"2022-05-27T14:15:33.542457Z","shell.execute_reply.started":"2022-05-27T14:15:32.181081Z","shell.execute_reply":"2022-05-27T14:15:33.541525Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"Weighted Average Probabilities (Soft Voting Classifier).\nIt seems that such a classification would be more accurate:","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n# call Soft Majority Rule classifier\npredictor(VotingClassifier(estimators=estimators, voting='soft'))\n\n# VotingClassifier results:\n# Accuracy: 93.2%\n# [[239  11]\n#  [ 23 227]]\n# Processed in  0.5797 seconds","metadata":{"execution":{"iopub.status.busy":"2022-05-27T14:15:33.546294Z","iopub.execute_input":"2022-05-27T14:15:33.546653Z","iopub.status.idle":"2022-05-27T14:15:34.875438Z","shell.execute_reply.started":"2022-05-27T14:15:33.546622Z","shell.execute_reply":"2022-05-27T14:15:34.874754Z"},"trusted":true},"execution_count":15,"outputs":[]}]}